# -*- coding: utf-8 -*-
"""19-04. bert_next_sentence_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a9PHRgv9s5DoXqhAErB9S8PVXfM7gEAC
"""

pip install transformers

"""## 1. 다음 문장 예측 모델과 토크나이저"""

import torch
from transformers import BertForNextSentencePrediction
from transformers import AutoTokenizer

model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

"""## 2. BERT의 입력"""

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
next_sentence = "pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand."

encoding = tokenizer(prompt, next_sentence, return_tensors='pt')

print(encoding['input_ids'])

print(tokenizer.cls_token, ':', tokenizer.cls_token_id)
print(tokenizer.sep_token, ':' , tokenizer.sep_token_id)

print(tokenizer.decode(encoding['input_ids'][0]))

print(encoding['token_type_ids'])

"""## 3. 다음 문장 예측하기"""

pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])
probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기
print(probs)

next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기
print('최종 예측 레이블 :', next_sentence_label)

# 상관없는 두 개의 문장
prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
next_sentence = "The sky is blue due to the shorter wavelength of blue light."
encoding = tokenizer(prompt, next_sentence, return_tensors='pt')

pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])
probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기
next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기
print('최종 예측 레이블 :', next_sentence_label)

"""## 4. 한국어 모델의 다음 문장 예측 모델과 토크나이저"""

import torch
from transformers import BertForNextSentencePrediction
from transformers import AutoTokenizer

model = BertForNextSentencePrediction.from_pretrained('klue/bert-base')
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

"""## 5. 다음 문장 예측하기"""

# 이어지는 두 개의 문장
prompt = "2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다."
next_sentence = "여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다."
encoding = tokenizer(prompt, next_sentence, return_tensors='pt')

pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])
probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기
next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기
print('최종 예측 레이블 :', next_sentence_label)

# 상관없는 두 개의 문장
prompt = "2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다."
next_sentence = "극장가서 로맨스 영화를 보고싶어요"
encoding = tokenizer(prompt, next_sentence, return_tensors='pt')

pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])
probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기
next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기
print('최종 예측 레이블 :', next_sentence_label)