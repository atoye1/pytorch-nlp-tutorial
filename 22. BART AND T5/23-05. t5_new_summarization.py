# -*- coding: utf-8 -*-
"""23-05. T5_new_summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I8IJQa1HvnhuMpfhQF48kM9k7v1EysXF
"""

pip install transformers

!pip install gdown

"""학습 데이터와 테스트 데이터 다운로드 방법은 e-book을 통해서만 제공합니다."""

import pandas as pd

DATA_TRAIN_PATH = 'summ_train.json'
train_df = pd.read_json(DATA_TRAIN_PATH)
train_df = train_df.dropna()
train_df = train_df[:40000]
len(train_df)

DATA_TEST_PATH = 'summ_test.json'
test_df = pd.read_json(DATA_TEST_PATH)
test_df = test_df.dropna()
test_df = test_df[:5000]
len(test_df)

def preprocess_data(data):
    outs = []
    for doc in data['documents']:
        line = []
        line.append(doc['media_name'])
        line.append(doc['id'])
        para = []
        for sent in doc['text']:
            for s in sent:
                para.append(s['sentence'])
        line.append(para)
        line.append(doc['abstractive'][0])
        line.append(doc['extractive'])
        a = doc['extractive']
        if a[0] == None or a[1] == None or a[2] == None:
            continue
        outs.append(line)

    outs_df = pd.DataFrame(outs)
    outs_df.columns = ['media', 'id', 'article_original', 'abstractive', 'extractive']
    return outs_df

# 원문과 요약문을 각각 'article_original'와 'abstractive'열에 저장.
train_data = preprocess_data(train_df)
train_data.head()

# 원문과 요약문을 각각 'article_original'와 'abstractive'열에 저장.
test_data = preprocess_data(test_df)
test_data.head()

# train_data의 첫번째 샘플의 article_original 열의 값 출력
train_data['article_original'].loc[0]

train_data['news'] = train_data['article_original'].apply(lambda x: ' '.join(x))
test_data['news'] = test_data['article_original'].apply(lambda x: ' '.join(x))

# train_data의 첫번째 샘플의 news 열의 값 출력
train_data['news'].loc[0]

train_data[['news', 'abstractive']].head()

import logging
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from transformers import T5ForConditionalGeneration, T5TokenizerFast
from tqdm import tqdm

class T5SummaryDataset(Dataset):
    def __init__(self, df, tokenizer, max_len, ignore_index=-100):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.docs = df
        self.len = self.docs.shape[0]
        self.pad_index = self.tokenizer.pad_token_id
        self.ignore_index = ignore_index

    def add_padding_data(self, inputs):
        if len(inputs) < self.max_len:
            pad = np.array([self.pad_index] * (self.max_len - len(inputs)))
            inputs = np.concatenate([inputs, pad])
        else:
            inputs = inputs[:self.max_len]
        return inputs

    def add_ignored_data(self, inputs):
        if len(inputs) < self.max_len:
            pad = np.array([self.ignore_index] * (self.max_len - len(inputs)))
            inputs = np.concatenate([inputs, pad])
        else:
            inputs = inputs[:self.max_len]
        return inputs

    def __getitem__(self, idx):
        instance = self.docs.iloc[idx]
        input_text = "summarize: " + instance['news']
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt", max_length=self.max_len, truncation=True).squeeze()
        input_ids = self.add_padding_data(input_ids)

        label_ids = self.tokenizer.encode(instance['abstractive'], return_tensors="pt", max_length=self.max_len, truncation=True).squeeze()
        label_ids = self.add_ignored_data(label_ids)

        return {'input_ids': np.array(input_ids, dtype=np.int_),
                'labels': np.array(label_ids, dtype=np.int_)}

    def __len__(self):
        return self.len

class T5ConditionalGeneration(torch.nn.Module):
    def __init__(self):
        super(T5ConditionalGeneration, self).__init__()
        self.model = T5ForConditionalGeneration.from_pretrained('paust/pko-t5-base')
        self.tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')
        self.pad_token_id = self.tokenizer.pad_token_id

    def forward(self, inputs):
        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()
        return self.model(input_ids=inputs['input_ids'],
                          attention_mask=attention_mask,
                          labels=inputs['labels'], return_dict=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = T5ConditionalGeneration().to(device)

batch_size = 8
max_len = 512
num_workers = 4
lr = 3e-5
max_epochs = 10
warmup_ratio = 0.1

tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')
train_dataset = T5SummaryDataset(train_data, tokenizer, max_len=max_len)
test_dataset = T5SummaryDataset(test_data, tokenizer, max_len=max_len)

train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)

optimizer = AdamW(model.parameters(), lr=lr)
total_steps = len(train_loader) * max_epochs
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=int(total_steps * warmup_ratio), T_mult=1, eta_min=0)

print('T5의 패딩 토큰 번호 :', tokenizer.pad_token_id)

print('첫번째 샘플의 원본 텍스트 :', train_data['news'].loc[0])

print('첫번째 샘플의 정수 인코딩과 패딩 후의 결과 :', train_dataset[0]['input_ids'])

print('첫번째 샘플의 정수 인코딩 결과를 텍스트로 복원 후 :')
tokenizer.decode(train_dataset[0]['input_ids'])

print('첫번째 샘플의 원본 요약문:')
train_data['abstractive'].loc[0]

print('첫번째 샘플의 요약문의 정수 인코딩 및 -100으로 패딩한 결과')
train_dataset[0]['labels']

# -100의 값은 tokenizer_decode를 하면 에러나므로 임시로 0으로 변경 후 출력
test_array = train_dataset[0]['labels']
test_array[test_array == -100] = 0
print('첫번째 샘플의 요약문의 정수 인코딩 결과를 텍스트로 복원 후 :')
tokenizer.decode(test_array)

best_loss = np.inf
for epoch in range(max_epochs):
    print(epoch+1, '수행 중')
    model.train()
    for batch in tqdm(train_loader, total=len(train_loader)):
        batch = {k: v.to(device) for k, v in batch.items()} # Move the batch tensors to the same device as the model
        optimizer.zero_grad()
        outputs = model(batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for batch in tqdm(test_loader, total=len(test_loader)):
            batch = {k: v.to(device) for k, v in batch.items()} # Move the batch tensors to the same device as the model
            outputs = model(batch)
            total_loss += outputs.loss.item()

    avg_loss = total_loss / len(test_loader)
    print(f'Epoch: {epoch+1}, Loss: {avg_loss}')

    # Save the best model
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), 'best_model.pt')
        print(f'Validation loss improved from {best_loss:.4f} to {avg_loss:.4f}. 체크포인트를 저장합니다.')

# 모델 인스턴스 생성
model_wrapper = T5ConditionalGeneration().to(device)

# 가중치 로드
model_wrapper.load_state_dict(torch.load('best_model.pt'))

# 모델을 평가 모드로 설정
model_wrapper.eval()

text = test_data.loc[25]['news']
text

text = "summarize: " + text

input_ids = tokenizer.encode(text)
tokenizer.decode(input_ids)

input_ids = tokenizer.encode(text)
input_ids = torch.tensor(input_ids)
input_ids = input_ids.unsqueeze(0).to(device)
output = model_wrapper.model.generate(input_ids, eos_token_id=1, max_length=512, num_beams=5)
output = tokenizer.decode(output[0], skip_special_tokens=True)
print(output)