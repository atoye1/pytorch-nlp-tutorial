# -*- coding: utf-8 -*-
"""22-02.bart_summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ge7LgBrnR2SuGQH14nILSP3eLjEAJLLR
"""

pip install transformers

!pip install gdown

"""학습 데이터와 테스트 데이터 다운로드 방법은 e-book을 통해서만 제공합니다.


"""

import pandas as pd

DATA_TRAIN_PATH = 'summ_train.json'
train_df = pd.read_json(DATA_TRAIN_PATH)
train_df = train_df.dropna()
train_df = train_df[:40000]
len(train_df)

DATA_TEST_PATH = 'summ_test.json'
test_df = pd.read_json(DATA_TEST_PATH)
test_df = test_df.dropna()
test_df = test_df[:5000]
len(test_df)

train_df.head()

test_df.head()

def preprocess_data(data):
    outs = []
    for doc in data['documents']:
        line = []
        line.append(doc['media_name'])
        line.append(doc['id'])
        para = []
        for sent in doc['text']:
            for s in sent:
                para.append(s['sentence'])
        line.append(para)
        line.append(doc['abstractive'][0])
        line.append(doc['extractive'])
        a = doc['extractive']
        if a[0] == None or a[1] == None or a[2] == None:
            continue
        outs.append(line)

    outs_df = pd.DataFrame(outs)
    outs_df.columns = ['media', 'id', 'article_original', 'abstractive', 'extractive']
    return outs_df

# 원문과 요약문을 각각 'article_original'와 'abstractive'열에 저장.
train_data = preprocess_data(train_df)
train_data.head()

# 원문과 요약문을 각각 'article_original'와 'abstractive'열에 저장.
test_data = preprocess_data(test_df)
test_data.head()

# train_data의 첫번째 샘플의 article_original 열의 값 출력
train_data['article_original'].loc[0]

train_data['news'] = train_data['article_original'].apply(lambda x: ' '.join(x))
test_data['news'] = test_data['article_original'].apply(lambda x: ' '.join(x))

# train_data의 첫번째 샘플의 news 열의 값 출력
train_data['news'].loc[0]

train_data[['news', 'abstractive']].head()

import logging
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
from tqdm import tqdm

class KoBARTSummaryDataset(Dataset):
    def __init__(self, df, tokenizer, max_len, ignore_index=-100):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_len = max_len
        # self.docs = pd.read_csv(file, sep='\t')
        self.docs = df
        self.len = self.docs.shape[0]

        self.pad_index = self.tokenizer.pad_token_id
        self.ignore_index = ignore_index

    def add_padding_data(self, inputs):
        # 인코더의 입력과 디코더의 입력은 0으로 패딩
        if len(inputs) < self.max_len:
            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))
            inputs = np.concatenate([inputs, pad])
        else:
            inputs = inputs[:self.max_len]

        return inputs

    def add_ignored_data(self, inputs):
        # 디코더의 레이블은 -100으로 패딩
        if len(inputs) < self.max_len:
            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))
            inputs = np.concatenate([inputs, pad])
        else:
            inputs = inputs[:self.max_len]

        return inputs

    def __getitem__(self, idx):
        instance = self.docs.iloc[idx]
        input_ids = self.tokenizer.encode(instance['news'])
        input_ids = self.add_padding_data(input_ids)

        label_ids = self.tokenizer.encode(instance['abstractive'])
        label_ids.append(self.tokenizer.eos_token_id)
        dec_input_ids = [self.tokenizer.eos_token_id]
        dec_input_ids += label_ids[:-1]
        dec_input_ids = self.add_padding_data(dec_input_ids)
        label_ids = self.add_ignored_data(label_ids)

        return {'input_ids': np.array(input_ids, dtype=np.int_),
                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),
                'labels': np.array(label_ids, dtype=np.int_)}

    def __len__(self):
        return self.len

class KoBARTConditionalGeneration(torch.nn.Module):
    def __init__(self):
        super(KoBARTConditionalGeneration, self).__init__()
        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1')
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
        self.pad_token_id = self.tokenizer.pad_token_id

    def forward(self, inputs):
        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()
        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()

        return self.model(input_ids=inputs['input_ids'],
                          attention_mask=attention_mask,
                          decoder_input_ids=inputs['decoder_input_ids'],
                          decoder_attention_mask=decoder_attention_mask,
                          labels=inputs['labels'], return_dict=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = KoBARTConditionalGeneration().to(device)

batch_size = 32
max_len = 512
num_workers = 4
lr = 3e-5
max_epochs = 10
warmup_ratio = 0.1

tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
train_dataset = KoBARTSummaryDataset(train_data, tokenizer, max_len=max_len)
test_dataset = KoBARTSummaryDataset(test_data, tokenizer, max_len=max_len)

train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)

optimizer = AdamW(model.parameters(), lr=lr)
total_steps = len(train_loader) * max_epochs
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=int(total_steps * warmup_ratio), T_mult=1, eta_min=0)

print('첫번째 샘플의 원문 텍스트 :', train_data['news'].loc[0])

print('첫번째 샘플의 원문 텍스트의 정수 인코딩 및 패딩 결과 :', train_dataset[0]['input_ids'])
print('정수 인코딩 및 패딩 후의 길이 :', len(train_dataset[0]['input_ids']))

print('첫번째 샘플의 정수 인코딩 후 복원 결과 :')
tokenizer.decode(train_dataset[0]['input_ids'])

print('첫번째 샘플의 요약문:', train_data['abstractive'].loc[0])

print('첫번째 샘플의 요약문 텍스트의 정수 인코딩 및 패딩 결과', train_dataset[0]['decoder_input_ids'])
print('정수 인코딩 및 패딩 후의 길이 :', len(train_dataset[0]['decoder_input_ids']))

print('첫번째 샘플의 정수 인코딩 및 패딩 후 복원 결과 :')
tokenizer.decode(train_dataset[0]['decoder_input_ids'])

# -100의 값은 tokenizer_decode하면 에러나므로 임시로 3으로 변경 후 출력
test_array = train_dataset[0]['labels']
test_array[test_array == -100] = 3
print('첫번째 샘플의 요약문 레이블 :', tokenizer.decode(test_array))

best_loss = np.inf
for epoch in range(max_epochs):
    print(epoch+1, '수행 중')
    model.train()
    for batch in tqdm(train_loader, total=len(train_loader)):
        batch = {k: v.to(device) for k, v in batch.items()} # Move the batch tensors to the same device as the model
        optimizer.zero_grad()
        outputs = model(batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for batch in tqdm(test_loader, total=len(test_loader)):
            batch = {k: v.to(device) for k, v in batch.items()} # Move the batch tensors to the same device as the model
            outputs = model(batch)
            total_loss += outputs.loss.item()

    avg_loss = total_loss / len(test_loader)
    print(f'Epoch: {epoch+1}, Loss: {avg_loss}')

    # Save the best model
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), 'best_model.pt')
        print(f'Validation loss improved from {best_loss:.4f} to {avg_loss:.4f}. 체크포인트를 저장합니다.')

'''
a100 gpu로 배치 크기 45로 수행하였을 때

1 진행 중
100%|██████████| 889/889 [07:36<00:00,  1.95it/s]
100%|██████████| 112/112 [00:25<00:00,  4.37it/s]
Epoch: 1, Loss: 1.5542645347969872
2 진행 중
100%|██████████| 889/889 [07:27<00:00,  1.99it/s]
100%|██████████| 112/112 [00:25<00:00,  4.43it/s]
Epoch: 2, Loss: 1.4970393446939332
3 진행 중
 93%|█████████▎| 831/889 [06:58<00:29,  2.00it/s]
Epoch: 3, Loss: 1.4749100123132979
4 진행 중
100%|██████████| 889/889 [07:35<00:00,  1.95it/s]
100%|██████████| 112/112 [00:25<00:00,  4.44it/s]
Epoch: 4, Loss: 1.4651334785989352
5 진행 중
100%|██████████| 889/889 [07:37<00:00,  1.94it/s]
100%|██████████| 112/112 [00:25<00:00,  4.37it/s]
Epoch: 5, Loss: 1.4626388241137778
6 진행 중
100%|██████████| 889/889 [07:35<00:00,  1.95it/s]
100%|██████████| 112/112 [00:25<00:00,  4.43it/s]
Epoch: 6, Loss: 1.4644672806773866
7 진행 중
100%|██████████| 889/889 [07:36<00:00,  1.95it/s]
100%|██████████| 112/112 [00:25<00:00,  4.44it/s]
Epoch: 7, Loss: 1.4712920082466943
8 진행 중
  4%|▍         | 36/889 [00:19<07:49,  1.82it/s]
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
/tmp/ipykernel_75844/3256950047.py in
     53
     54 if __name__ == '__main__':
---> 55     train()

/tmp/ipykernel_75844/3256950047.py in train()
     32             outputs = model(batch)
     33             loss = outputs.loss
---> 34             loss.backward()
     35             optimizer.step()
     36             scheduler.step()

/opt/conda/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    305                 create_graph=create_graph,
    306                 inputs=inputs)
--> 307         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
    308
    309     def register_hook(self, hook):

/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    152         retain_graph = create_graph
    153
--> 154     Variable._execution_engine.run_backward(
    155         tensors, grad_tensors_, retain_graph, create_graph, inputs,
    156         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag

KeyboardInterrupt:
'''

'''
# 모델 인스턴스 생성
model_wrapper = KoBARTConditionalGeneration().to(device)

# 가중치 로드
model_wrapper.load_state_dict(torch.load('best_model.pt'))

# 모델을 평가 모드로 설정
model_wrapper.eval()
'''

text = test_data.loc[25]['news']
text

label = test_data.loc[25]['abstractive']
label

'''
input_ids = tokenizer.encode(text)
input_ids = torch.tensor(input_ids)
input_ids = input_ids.unsqueeze(0).to(device)
output = model_wrapper.model.generate(input_ids, eos_token_id=1, max_length=512, num_beams=5)
output = tokenizer.decode(output[0], skip_special_tokens=True)
print(output)
'''

!pip install konlpy

!pip install rouge

from konlpy.tag import Okt
from rouge import Rouge

def calculate_rouge_for_korean_sentences(reference_sentence, hypothesis_sentence):
    # 형태소 분석기 초기화
    okt = Okt()

    # 형태소 분석을 통한 토큰화
    def tokenize_and_concat(text):
        return ' '.join(okt.morphs(text))

    # 두 문장 토큰화
    tokenized_reference = tokenize_and_concat(reference_sentence)
    tokenized_hypothesis = tokenize_and_concat(hypothesis_sentence)

    # ROUGE 점수 계산
    rouge = Rouge()
    scores = rouge.get_scores(tokenized_hypothesis, tokenized_reference)
    return scores

# 사용 예시
label = "고양이가 매트 위에 앉아 있다."
model1_prediction = "매트 위에 고양이가 앉아 있다."
model2_prediction = "고양이가 매트 위에 앉아 있다."

rouge_scores = calculate_rouge_for_korean_sentences(label, model1_prediction)
print(rouge_scores[0]['rouge-l']['f'])
rouge_scores = calculate_rouge_for_korean_sentences(label, model2_prediction)
print(rouge_scores[0]['rouge-l']['f'])

output = '배우 배수지가 매니지먼트 숲과 전속계약을 체결해 배우 배수지의 장점과 매력을 극대화할 수 있는 작품 선택부터 국내외 활동, 가수로서의 솔로 활동까지 활발하게 이루어질 수 있도록 지원할 예정이다.'
label = test_data.loc[25]['abstractive']
rouge_scores = calculate_rouge_for_korean_sentences(label, output)

print('모델의 예측:', output)
print('정답 문장:', label)
print('rouge score:', rouge_scores[0]['rouge-l']['f'])