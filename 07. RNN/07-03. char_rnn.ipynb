{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Char RNN"
      ],
      "metadata": {
        "id": "3D2HtbthPC8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7RK-tfDJNPXr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30sGoxr6ObQ4",
        "outputId": "e5abe45e-ce56-44c2-fd72-7f826eeec3e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "ReFOwPxkOb-f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1uzA4gPOcwo",
        "outputId": "68336d21-25dd-44c5-e18d-2bf1b14efda0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C111iVdOd5g",
        "outputId": "0bc818cf-5a32-47f6-ee5c-a22db1ed7e34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFTHM-r2OeuP",
        "outputId": "121e27da-a8ac-4ef3-891f-75b1ba940660"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuByU5NHOfdv",
        "outputId": "d7d96d30-8b8c-4919-f0c1-c75ae9fa6fab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZIoBrVLOgsf",
        "outputId": "7f34eeb6-9378-465c-900e-1d5755f4cac4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Nrk3RmOho3",
        "outputId": "828333f9-4da2-46f5-a8d6-720a423cba9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c1bfbd518a63>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUpOx9MfOimn",
        "outputId": "b84f651d-5d48-4db7-b56c-c36d1f5c0efa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3UCadLHEOjjP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "DvBdiPPjOkcf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obkF6QQmOlTX",
        "outputId": "4d823547-6232-4c54-ec7f-c6f286ebf75f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV1ArVljOmB3",
        "outputId": "4ca681b9-bf22-41fa-bfe4-eb6233020e7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWI3pEV1Om1_",
        "outputId": "6a799ccc-fba1-44ba-933b-f198da3f34c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "wzs_kCYZOntH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52HabE7yOowH",
        "outputId": "e4f4966e-32ff-418d-8568-9ef7bada869e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.7251660823822021 prediction:  [[2 2 2 2 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeeel\n",
            "1 loss:  1.465484857559204 prediction:  [[2 2 2 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeee!\n",
            "2 loss:  1.3028860092163086 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "3 loss:  1.1449304819107056 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "4 loss:  0.9963593482971191 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.8365715146064758 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "6 loss:  0.69331294298172 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.5695310831069946 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.4524063467979431 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.3463137149810791 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.253948450088501 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.18432354927062988 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.13092008233070374 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.09303911030292511 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.06515374779701233 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.04514053091406822 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.031547874212265015 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.022425498813390732 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.016360323876142502 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.01234408188611269 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.009605982340872288 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.007655061781406403 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.006212727166712284 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.005120151210576296 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.004279990214854479 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.003626913530752063 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0031143962405622005 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.002707734005525708 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0023811454884707928 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0021155495196580887 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.001896694884635508 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0017143689328804612 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0015611748676747084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0014315620064735413 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.001321281073614955 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.001227245549671352 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0011465564602985978 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.00107705092523247 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0010167077416554093 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.000964052218478173 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0009176809107884765 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0008766181999817491 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0008399600046686828 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0008071112679317594 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0007774294936098158 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0007507005357183516 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0007264484302140772 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0007044588564895093 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0006844463059678674 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0006662438972853124 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.000649613793939352 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0006343892309814692 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0006204271921887994 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0006075372803024948 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0005955764790996909 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0005844020051881671 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0005740851047448814 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0005642924807034433 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0005551193025894463 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0005465418216772377 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0005383930983953178 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0005307924002408981 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0005235729040578008 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.000516663130838424 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0005101583665236831 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0005039871903136373 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0004981494857929647 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0004925023531541228 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.00048716505989432335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0004820182512048632 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0004770382365677506 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.00047232024371623993 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.00046774520887993276 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.000463289296021685 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.00045895250514149666 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0004548300930764526 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0004508030542638153 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.000446895050117746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.00044313003309071064 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.00043941271724179387 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.00043583830120041966 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0004323353641666472 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0004289515782147646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.00042561543523333967 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.000422374636400491 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0004192052292637527 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0004161073884461075 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.00041305721970275044 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0004100784717593342 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0004071712610311806 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.00040424015605822206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.00040140439523383975 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0003986877854913473 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0003959711757488549 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0003932545077987015 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.00039063318399712443 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.00038805956137366593 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0003855334944091737 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.00038303135079331696 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.00038050528382882476 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 더 많은 데이터로 학습한 문자 단위 RNN(Char RNN)"
      ],
      "metadata": {
        "id": "lsUQYdIcOrXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "WOW21zBvOpun"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "uGf1RQCtOsgf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "metadata": {
        "id": "Sa9M0mm-Otpn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeDEOZiGOu0A",
        "outputId": "8cb537e8-cb30-4a62-bdc9-070bc26c82d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'s': 0, 'f': 1, 'u': 2, '.': 3, 'y': 4, 'b': 5, 'l': 6, ' ': 7, \"'\": 8, 'p': 9, ',': 10, 'g': 11, 'i': 12, 'h': 13, 'r': 14, 'n': 15, 'e': 16, 't': 17, 'a': 18, 'o': 19, 'm': 20, 'w': 21, 'c': 22, 'k': 23, 'd': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JagIF2jtOvwX",
        "outputId": "0748b5f6-4d11-4063-d2cb-76d7883bb397"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "9E8mwyykOwlf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8LS89qqOx2v",
        "outputId": "79555b01-17bb-4224-9ca8-f608ea68376e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg-D-BTUOy5X",
        "outputId": "a96dc494-235e-4e9c-8514-82e12752f119"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 1, 7, 4, 19, 2, 7, 21, 18, 15]\n",
            "[1, 7, 4, 19, 2, 7, 21, 18, 15, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "Ccf4slS2Oz13"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biK51T5oO0n3",
        "outputId": "69da2cbd-9db4-4ec2-87a3-b07d08c7d820"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_FaeEYLO1z3",
        "outputId": "4f822c51-ded9-4cb2-f1bd-521df2996c93"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-Pt4L3zO263",
        "outputId": "6170d5a7-0f48-4a33-ea49-0477f20c1211"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  7,  4, 19,  2,  7, 21, 18, 15, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "N5uxfwaoO3zf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "metadata": {
        "id": "R6yIpfAhO4vu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "OoDSJtnmO5lX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PB9jJRtO7Cn",
        "outputId": "83269a45-2e10-4de6-fb82-16c4ddfdcc90"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BviiylmuO8LH",
        "outputId": "082e055c-c1a8-4178-e943-18dadbc15326"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNm1crcWO9CP",
        "outputId": "4900ec9d-9109-4333-d6ae-a274bbb94169"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnphDRv3O97H",
        "outputId": "3a3d8dc5-3b2a-4890-95f1-91c05180fba6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mrmmrmmrrmrmmrmrmmmmmmrrrmmmmmmmrmmmmmmmrrmrmmmmrmmrmmmmmmmmrmrmmrmmmmrrmmmmrmmmmmrmmmrmmmmrmrmmmrmrmrmmrmmmrmmmmmmmmrmmrmmmmmmrmmmrmmmrrmmrmrmrmmmmrmmmrmmmmmmmmmmrrmmmmmmmmrmmrrm\n",
            "                                                                                                                                                                                   \n",
            "tt.h u.h tutuf...utuu.tu..utuu.tutuuuu.tu.utu.ftu..tu..uuut..uutuutt.ut.fttuuuu.ftfuuuuu.utu.utuftu.tuu.ttfuuuuuuuftuu.tu..tt..utt..tuftfftuu.tt.uutuuutuf.tuuftuuuuutu.uttu.utuuuu\n",
            "                                                                                                                                                                                   \n",
            " leleleleleleolllt el eltlelelelelelelelel leelellltleltl ll alell lltlel aetltlelelelelelele l llle lelelelelel lelelelelllelel llell llele lelllll lalelelelelelele lllee  ell tl\n",
            " tttn'sa l l tl   lll ll l lll   l   l lltltlllll    ll  lll ml ltt  ll tlll l   l ltl lllllttltt  mtltl ltl l lll l l l l lltml ttll ll lttlltt l ttll  l l ltl tlllll tlltlllt  l\n",
            "  mm msm    m      m      m t  mt     m m         m  m        m m  m     m mm t  mt   m   m m mm m m mt m t   t t   t          m     m m    m    mt  mt  m  m     mm        m mt m \n",
            "     i      h   h                 h h                                    h                                              i   h            h              h                          \n",
            "      t h   h   h           h     h    t   t   h  h    hh  h  h   h               d o   h             h h     hn  h d    n    h       t   t               d h         h            \n",
            "t o   t  o   t      t t  t     od      t  ht       t   t  oot    to  ot   t   t   d          eo t tod dt        ht     h   t to       ththt  t t      do   o   t         o   t    t\n",
            "tett eto o  otte  e toeotte  etot eh o to oeo teot eot  o eoto tee  tee t to  eot e tot  e t eo tot   eot eot eohtt eoth ott eottooe  ootoe et e  eotoe    ottee   o   e   o t  o t\n",
            "tette tot   oeo o eoth      aeeo   ot   o oeo   ot     eo  oe eoee t e     os e to  e      tee  toto   o   oe e tos a t    toeo eooeoe o  e etee  eot t     t      e   e   oseoe   \n",
            "t d   th    o      otm      ao      t       r    t         o      r    t    s    o    d    toe  tot             tos   th   toe  tooto  o   r  ae   ot                    oto e t   \n",
            "t d not     ot d   otmd     ooto   m m tot  r   ot      odto  s   rt  rt   ot   t    mt   dtoemdtot d  rd trs t ton trt  r to rdtoot  to  e dto n tot dm   rd   r  o  o totos  d   \n",
            "d d n to   oo  s   otmd   o do h  tm o to 'on    t   o   sto  o   md t n  tnn d n   tm    oto mdton   dns ton o oon tmth rdto rd o    to  e d o n to 'dmd  rd   r  d  o o  os  d   \n",
            "t d n tos  to hsn  otmd   o do h  to o to  on    to  o  m to  o   md to   tns t     tnt   oto mdto    tns to  e oot  mt  r to  o o    to  e g o   to  d    nd tor     o oo o   d n \n",
            "t tos tos  to hamg otmd     to h  to ' to  on  e to  eh m to  o  emd tor  tns t  '  tot    to m to  e tns to  e toth mt  m to  o t    to  e g t   to mt s  ng tos n   oeoo to  t   \n",
            "t tor tos  to os   otndo  o to 'e to m to ton  e to  e em to  o  ens tor  tns to '  tos   oto m the e tns toe e to 'ame en to  o to m to  o  eto  to ct s ens tos n   oeto to mt   \n",
            "t tor aos  to da   mtndoeco toe's toem torton  e to  aher th  o  ers aor  tns toe e tns   heher th he ans tor e tu  amt en to coeto m th tor  tor toecans ens aos n   heto thert   \n",
            "t aor eos  to  s   otndoecs toe'd tmtn to ter  e do  eher th  o  ert eor  tns toe'  tos   sthem th 'e dns tor e tus aot er to chethem th lord tor themtns ans tos n   heto themd  c\n",
            "d tor  ond to 'u  a tnd ecs to 'd to m to torg e do  ehem th do  e d eon  tns to 'd tns   sthem th 'e tns aor s tui'a nhem th chedhem th le d tor ahemtnd ans tos n   heto themt  c\n",
            "t tori ond to du  e tntoec  to 'd to m to bord enton rhem tonlo lerd eoni tnd tos'e tos    them th 's tnd tor e torlaouhem toncheth m th lord tor themtnd ens io  ns i eto themton \n",
            "toton tond to buice tntoecs ton't tm m to tort enton ehem to lo lemt tond tnd ton't tns g dtoem ton', tnd torls tutla them tonchethem to to d tor th mtnd tns tos n  i etr themt nc\n",
            "touon tond to buile tnt ems ton't tm m to to g e toe nhem to lo lemt to p tns ton't tns gnstoem to 's gnd torks tut a them to ch them to to d torkto mtnd ens to  ns g eto toemt nc\n",
            "touon tond toebuice t toecs ton't tmum toetoom e toe them to bo lemt to p t s ton't tosignsthem tos's end tonks tut t them tonchethem tosle d torktoemtnd e s to e s g eoo themt n \n",
            "toton wond toebuice t thecs ton't toum toetoom e toe ther to bo le t eo p ans ton't tosiwnsthem tos's wnd wonks tut a ther toechether tosbe d torkthe tnd gns womens w eoo thert  c\n",
            "totor tond toebuice t thecs tot't tmum tneloodle the ther theco lert wondlans ton't ursignsther tos's wnd wonks tut a ther to checher tosbe g torktoertnd ens wnsens i ear thert ec\n",
            "letoo wond toecuice t theco ton't toum tnetondle the ther theco le t wondland won't tosigngther thsks wnd wonks tut a ther thechether to ceng tor thertnd ens wnmens w ean thert ec\n",
            "t toriiond to cuice t theco ton't toum toetondle the ther theco le t wondlans ton't tosign ther tosks wnd tonks tut a ther tonch ther toscong tor toertnd ens wnsens i ean thert ec\n",
            "i toriiond to buile t th bs ton't amum toetool e to  th m th co lemt wond a s ton't a sign ther tosks ind tonk, tut aoth r to cs th r toslong tor toertnd ens inmens g eon th rs ec\n",
            "iouot wond to butle t thebs ton't toum toetoot e toe them th bo lectotorlltnd ton't tosign them tosks dnd tonk, tut uoth m torcs them toslong torktoersnd e s wose s i eoo themshnc\n",
            "iouoriwond to buile t uhebs ton't trum tn tort e the th m th bo lemt wond and won't ansign them tosks dnd wonk, tut a th m thech bh m to long por themend ens wmmens g ean them  ec\n",
            "touou wond to buile t theb, ton't toum tnetootle the them th bo lemt tond and won't tosign ther tosk, dnd wonk, tut a them thech bhem th long tor ther nd ens wmme sigyeto themshec\n",
            "pouot wond wo buile t theps ton't trum tn lootle to  them th bo lect wond and won't tnsign ther tosks wnd wonk, tut r th r thech them to lo g tor the snd ens wmmensigyetf thershrc\n",
            "pouou iond to buila a uheps ton't arum to leople to ethem th bo lect wond and won't amsign ther tosks dnd wonk, tut roth r toech bhem to long tor the sndlens wpmensigyeoo ther hep\n",
            "pouou iond to buila a uhep, don't doum to leople t eethem th bo lect wond and won't dosign ther tosks dnd wonk, tut roth r thech bhem to long tor ther ndle s wmmensiiyeoo thers ec\n",
            "pouor'wond to buila t th p, don't drum tn leople togeth m th co lect wond tnd won't dmsigngther tosks dnd wonk, dut roth r torch bh m to long dorktherendle s wmmens iyeof thers ec\n",
            "touor wonb to buila a thep, don't drum tn teople togethem to lo lect wond and don't dnsign them tosks dnd donk, dut roth r tosch them to long torktherendle s wmmensiiyeoo thersenc\n",
            "touou wont to buila a a ep, don't arum t  leo le tog them to lo lect wond and won't ansign them tosks and wonk, tut rothem toach th m to long tor therensle s wmmensiiyeof themeerc\n",
            "touou wont to build a uh p, don't toum tn leople to ethem to bollect wond and won't tosign the  tasks and wonk, tut roth m toach them to long tor themendlens wmmensiiyeof themehel\n",
            "tyuor wpnt wo luild a ah p, don't drum tm leople to eth m th lo lemt wond and won't dmsign them tasks and work, dut rothem thrch dh m to long dor the endlens wmmensiiy of themseep\n",
            "touou wont to build a ahip, don't drum tp teople aogether th co lect wood and don't dosign the  tosks dnd donk, dut rother thech them to long dor the endlens wmmensiiyeoo thershep\n",
            "poeou wont to build a ahip, don't drum tp beople aogether to collect wood and won't dssign the  tosks dnd wonk, but rother toech them to long for therendleps wmmensiiy of thershep\n",
            "pyeou ipnd to build a ship, don't drum tp leople aog ther to lollect wood and won't dssign ther tasks dnd work, but rother toach them to long for therendleps wmmensiiy of thers ed\n",
            "pyeou want to build a ship, don't drum tp beople aog ther th lollect wood and don't dnsign ther tasks and work, but rather thach them to long for therendleps wmmensity of thersead\n",
            "pyeou want to build a ship, don't arum tp beople ao ether to bollect wo d and won't assign them tosks and wo k, but rather toachethem to long for themendleps wmmensity of thersend\n",
            "peeot want to build a ship, don't arum tp beople aogether to collect wood and won't assign them tasks and work, but rathem toachethem to long for the endless wmmensity of thersen \n",
            "peeou want to build a ship, don't drum tp people aogether th collect wood and don't dssign them tasks and work, but rother thachethem to long for the endless wmmensity of thersen \n",
            "p eou wont to build a ship, don't drum tp people together th collect wood and don't dssign them tasks and work, but rothem thach them to long for themendleps wmmensiiy of therseap\n",
            "p eou want to build a ship, don't drum us people sogether to collect wood and don't dssign them tosks and work, but rothem toach them to long for themendless immensiiy of thership\n",
            "p eou wont to build a ship, don't arum us people together to collect wood and don't assign them tasks and work, but rathem toach them to long for themendless immensity of themseip\n",
            "p eou want to build a ship, don't drum up people together th collect wood and don't dssign them tasks and work, but rather thach them to long for themendless immensity of themseal\n",
            "p eou want to build a ship, don't drum tp people together to bollect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of therseap\n",
            "p eou want to build a ship, don't drum up people together to lollect wood and don't dssign them tasks and work, but rather toach bhem to long for the endless immensity of themseap\n",
            "p eou want to build a ship, don't drum up people together to lollect wood and don't dssign them tasks and work, but rather toach them to long for themendless immensity of themseip\n",
            "p oou want to build a ship, don't arum up people together to lollect wood and don't assign them tasks and work, but rather toach them to long for themendless immensity of themseay\n",
            "p eou want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather toach them to long for themendless immensity of themseip\n",
            "p eou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for themendless immensity of themseip\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign the  tasks and work, but rather toach them to long for the endle s immensity of themseap\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather thach the  to long for the endless immensity of the seap\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for themendless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for themendless immensity of the seip\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for themendless immensity of the seap\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together th collect wood and don't dssign ther tasks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather thach them to long for the endless immensity of thereeap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum us people together to collect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to bollect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather thach them to long for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together th collect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the seap\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather thach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seap\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvgNf3GsO_GH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}