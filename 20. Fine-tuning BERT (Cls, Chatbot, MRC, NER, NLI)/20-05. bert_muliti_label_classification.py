# -*- coding: utf-8 -*-
"""20-05. bert_muliti_label_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE5FF_6nIRWVYbe811EVwh3u_aXnHoRl

References : https://github.com/adlnlp/K-MHaS

# 한국어 혐오 발언 다중 레이블 분류 : K-MHaS (Korean Multi-label Hate Speech Dataset)
"""

!pip install transformers
!pip install datasets

"""## 데이터셋 로드 및 구조 확인"""

from datasets import load_dataset

dataset = load_dataset("jeanlee/kmhas_korean_hate_speech")

dataset

dataset = load_dataset("jeanlee/kmhas_korean_hate_speech", split="test")

dataset

dataset.features

print('테스트 데이터 셋의 크기 :', len(dataset['text']))
print('첫번째 샘플 출력 :', dataset['text'][0])
print('첫번째 샘플의 레이블 출력 :', dataset['label'][0])

"""- 데이터셋의 [깃허브](https://github.com/adlnlp/K-MHaS)로부터 확인할 수 있는 각 레이블이 의미하는 바는 다음과 같습니다. 레이블은 0부터 8까지 총 9개의 레이블이 존재합니다.

      class_label:
        names:
          0: origin (출신차별)
          1: physical (외모차별)
          2: politics (정치성향차별)
          3: profanity (혐오욕설)
          4: age (연령차별)
          5: gender (성차별)
          6: race (인종차별)
          7: religion (종교차별)
          8: not_hate_speech (혐오아님)
"""

print('두번째 샘플 출력 :', dataset['text'][1])
print('두번째 샘플의 레이블 출력 :', dataset['label'][1])

"""## 데이터셋 전처리"""

import pandas as pd
import numpy as np
import datetime
from tqdm import tqdm
import torch

# BERT 사용을 위함
from transformers import BertTokenizer
from transformers import BertForSequenceClassification, AdamW, BertConfig
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# for padding
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 전처리 및 평가 지표
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score

# 훈련 데이터, 검증 데이터, 테스트 데이터를 로드합니다.

train = load_dataset("jeanlee/kmhas_korean_hate_speech", split="train")
validation = load_dataset("jeanlee/kmhas_korean_hate_speech", split="validation")
test = load_dataset("jeanlee/kmhas_korean_hate_speech", split="test")

# 훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 `[CLS] 문장 [SEP]` 구조를 만듭니다.

train_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', train['text']))
validation_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', validation['text']))
test_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', test['text']))

# 정답인 레이블의 위치에는 1, 나머지 위치에는 0을 기록합니다.
# 레이블 전처리 예시)
# [8]    -> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] : 의미적으로는 [no, no, no, no, no, no, no, no, no, yes]
# [2, 3] -> [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] : 의미적으로는 [no, no, yes, yes, no, no, no, no, no, no]

from sklearn.preprocessing import MultiLabelBinarizer

enc = MultiLabelBinarizer()

def multi_label(example):
    enc_label = enc.fit_transform(example['label'])
    float_arr = np.vstack(enc_label[:]).astype(float)
    update_label = float_arr.tolist()
    return update_label

train_labels = multi_label(train)
validation_labels = multi_label(validation)
test_labels = multi_label(test)

test_sentences[:5]

# 각 레이블은 기존에 [8], [2, 3], [2], [0], [8] 이었으며 전처리 후 아래와 같이 변경됨.
test_labels[:5]

"""## BERT 토크나이저를 이용한 전처리"""

# 한국어 BERT 중 하나인 'klue/bert-base'를 사용.
tokenizer = BertTokenizer.from_pretrained('klue/bert-base')

max_len = 128

def data_to_tensor (sentences, labels, max_len):
  # 정수 인코딩 과정. 각 텍스트를 토큰화한 후에 Vocabulary에 맵핑되는 정수 시퀀스로 변환한다.
  # ex) ['안녕하세요'] ==> ['안', '녕', '하세요'] ==> [231, 52, 45]
  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]
  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]

  # pad_sequences는 패딩을 위한 모듈. 주어진 최대 길이를 위해서 뒤에서 0으로 채워준다.
  # ex) [231, 52, 45] ==> [231, 52, 45, 0, 0, 0]
  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype="long", truncating="post", padding="post")

  attention_masks = []

  for seq in input_ids:
      seq_mask = [float(i > 0) for i in seq]
      attention_masks.append(seq_mask)

  tensor_inputs = torch.tensor(input_ids)
  tensor_labels = torch.tensor(labels)
  tensor_masks = torch.tensor(attention_masks)

  return tensor_inputs, tensor_labels, tensor_masks

train_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels, max_len)
validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels, max_len)
test_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels, max_len)

tokenizer.decode([2])

tokenizer.decode([3])

print('정수 인코딩 결과:', test_inputs[0])
print('-' * 20)
print('원본 문장 복원 결과:', tokenizer.decode(test_inputs[0]))
print('-' * 20)
print('어텐션 마스크:', test_masks[0])
print('-' * 20)
print('샘플의 길이:', len(test_inputs[0]))
print('-' * 20)
print('레이블:', test_labels[0])

batch_size = 64

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

test_data = TensorDataset(test_inputs, test_masks, test_labels)
test_sampler = RandomSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

print('훈련 데이터의 크기:', len(train_labels))
print('검증 데이터의 크기:', len(validation_labels))
print('테스트 데이터의 크기:', len(test_labels))

"""## GPU가 정상 셋팅되었는지 확인.  
Colab에서 GPU를 사용하기 위해서는 아래와 같이 설정이 되어있어야만 합니다.  

* 런타임 > 런타임 유형 변경 > 하드웨어 가속기 > 'GPU' 선택
"""

if torch.cuda.is_available():
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print('No GPU available, using the CPU instead.')

"""## 모델 로드하기"""

num_labels = 9

model = BertForSequenceClassification.from_pretrained("klue/bert-base", num_labels=num_labels, problem_type="multi_label_classification")
model.cuda()

# 옵티마이저 선택
optimizer = AdamW(model.parameters(), lr = 2e-5)

# 몇 번의 에포크(전체 데이터에 대한 학습 횟수)를 할 것인지 선택
epochs = 4

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))

    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

def multi_label_metrics(predictions, labels, threshold=0.5):
    """
    멀티레이블 분류 문제의 예측값과 실제 라벨을 기반으로 다양한 평가 지표를 계산합니다.
    이미 확률로 변환된 예측값을 사용하여, 임계값을 기준으로 레이블을 결정하고 평가 지표를 계산합니다.

    Parameters:
    predictions (numpy.ndarray): 모델에 의해 예측된 확률값. 각 샘플에 대한 레이블 확률을 포함합니다.
    labels (numpy.ndarray): 실제 라벨값. 각 샘플에 대한 실제 레이블을 포함합니다.
    threshold (float): 레이블을 결정하기 위한 임계값. 기본값은 0.5입니다.

    Returns:
    dict: 다양한 평가 지표(metrics)에 대한 값들을 담은 사전.
    """

    # 임계값을 넘는 확률 값을 가진 경우 1로 예측했다고 간주합니다.
    y_pred = (predictions >= threshold).astype(int)

    # 사용 가능한 메트릭들을 사용합니다.
    accuracy = accuracy_score(labels, y_pred)
    f1_macro_average = f1_score(y_true=labels, y_pred=y_pred, average='macro', zero_division=0)
    f1_micro_average = f1_score(y_true=labels, y_pred=y_pred, average='micro', zero_division=0)
    f1_weighted_average = f1_score(y_true=labels, y_pred=y_pred, average='weighted', zero_division=0)
    roc_auc = roc_auc_score(labels, y_pred, average='micro')

    # 메트릭 결과에 대해서 리턴합니다.
    metrics = {'accuracy': accuracy,
               'f1_macro': f1_macro_average,
               'f1_micro': f1_micro_average,
               'f1_weighted': f1_weighted_average,
               'roc_auc': roc_auc}

    return metrics

"""## 모델 학습"""

def train_epoch(model, train_dataloader, optimizer, device):
    """
    하나의 에포크 동안 모델을 학습시키는 함수입니다.

    Parameters:
    model (torch.nn.Module): 학습시킬 모델 객체.
    train_dataloader (torch.utils.data.DataLoader): 학습 데이터셋의 DataLoader.
    optimizer (torch.optim.Optimizer): 최적화 알고리즘을 구현하는 객체.
    device (torch.device): 학습에 사용할 장치(CPU 또는 CUDA).

    Returns:
    float: 평균 학습 손실값.
    """

    total_train_loss = 0  # 학습 손실을 누적할 변수 초기화
    model.train()  # 모델을 학습 모드로 설정

    # 학습 데이터로더를 순회하며 배치 단위로 학습
    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc="Training Batch"):
        batch = tuple(t.to(device) for t in batch)  # DataLoader에서 배치를 받아 각 텐서를 지정된 장치로 이동
        b_input_ids, b_input_mask, b_labels = batch  # 배치에서 입력 ID, 마스크, 라벨 추출

        # 모델에 배치를 전달하여 손실값 계산
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)

        # 손실값 추출
        loss = outputs.loss

        optimizer.zero_grad()  # 그래디언트 초기화
        loss.backward()  # 역전파를 통해 그래디언트 계산
        optimizer.step()  # 매개변수 업데이트

        total_train_loss += loss.item()  # 총 손실에 더함

    avg_train_loss = total_train_loss / len(train_dataloader)  # 평균 학습 손실 계산

    return avg_train_loss  # 평균 학습 손실 반환

def evaluate(model, validation_dataloader, device):
    """
    모델을 사용하여 검증 데이터셋에 대한 평가를 수행하는 함수입니다. 이 함수는 멀티레이블 분류 문제에 적합하게 설계되었습니다.
    각 레이블에 대한 예측을 독립적으로 수행하고, 여러 평가 지표를 계산하여 모델의 성능을 평가합니다.

    Parameters:
    model (torch.nn.Module): 평가할 모델 객체. 이 모델은 멀티레이블 분류를 위한 출력을 생성해야 합니다.
    validation_dataloader (torch.utils.data.DataLoader): 검증 데이터셋의 DataLoader.
    이 DataLoader는 입력 데이터와 정답 레이블을 배치로 제공합니다.
    device (torch.device): 평가에 사용할 장치(CPU 또는 CUDA).

    Returns:
    float: 평균 검증 손실값. 검증 데이터셋에 대한 모델의 손실을 평균낸 값입니다.
    dict: 다양한 평가 지표(metrics)에 대한 값들을 담은 사전. 이 사전은 정확도(accuracy),
    F1 점수(macro, micro, weighted) 및 ROC-AUC 점수를 포함합니다.
    """

    model.eval()  # 모델을 평가 모드로 설정

    total_eval_loss = 0
    predictions, true_labels = [], []  # 모델 예측 확률과 실제 라벨을 저장할 리스트

    for batch in validation_dataloader:
        batch = tuple(t.to(device) for t in batch)  # 배치 데이터를 디바이스로 이동
        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():  # 그래디언트 계산을 수행하지 않음
            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)

        loss = outputs.loss
        total_eval_loss += loss.item()  # 총 손실에 더함

        logits = outputs.logits.detach().cpu().numpy()  # 모델 예측값(로짓)을 numpy 배열로 변환
        label_ids = b_labels.to('cpu').numpy()  # 실제 라벨값을 numpy 배열로 변환

        # 시그모이드 함수를 적용하여 로짓을 확률로 변환
        # 멀티레이블 분류에서는 각 레이블에 대한 독립적인 예측을 수행합니다.
        # 예를 들어, logits 값이 [-1.2, 2.3, -0.4, 1.0, -2.2, 0.5, -0.7, 1.2, 0.3]로 주어지고,
        # 각각의 레이블에 대해 시그모이드 함수를 적용하면, 확률은
        # probs = [0.23, 0.91, 0.40, 0.73, 0.10, 0.62, 0.33, 0.77, 0.57]과 같이 계산됩니다.
        # 실제 예측 = [0 1 0 1 0 1 0 1 1]
        sigmoid = torch.nn.Sigmoid()
        probs = sigmoid(torch.Tensor(logits)).numpy()

        predictions.extend(probs)
        true_labels.extend(label_ids)

    # multi_label_metrics 함수는 멀티레이블 평가 지표 계산을 합니다.
    # probs = [0.23, 0.91, 0.40, 0.73, 0.10, 0.62, 0.33, 0.77, 0.57] 이라고 가정하였을 때,
    # 여기서 임계값(예: 0.5)을 넘는 확률 값을 가진 레이블은 해당 샘플에 존재한다고 예측합니다.
    # 위 예시에서는 레이블 2, 4, 6, 8, 9가 임계값을 초과하므로, 모델의 예측 레이블은 [2, 4, 6, 8, 9]가 됩니다.
    eval_metrics = multi_label_metrics(np.array(predictions), np.array(true_labels))

    avg_eval_loss = total_eval_loss / len(validation_dataloader)

    return avg_eval_loss, eval_metrics

# 최소 검증 손실 초기화
min_val_loss = float('inf')

# 메인 학습 & 평가 루프
for epoch_i in range(0, epochs):
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))

    # 학습 단계
    train_epoch(model, train_dataloader, optimizer, device)

    print("\nRunning Validation...")
    # 검증 단계
    avg_val_loss, eval_metrics = evaluate(model, validation_dataloader, device)
    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Accuracy: {0:.2f}".format(eval_metrics['accuracy']))
    print("  F1 Macro: {0:.2f}".format(eval_metrics['f1_macro']))
    print("  F1 Micro: {0:.2f}".format(eval_metrics['f1_micro']))
    print("  F1 Weighted: {0:.2f}".format(eval_metrics['f1_weighted']))

    # 검증 손실이 현재까지의 최소값보다 작은 경우 체크포인트 저장
    if avg_val_loss < min_val_loss:
        print(f"Validation loss decreased ({min_val_loss:.2f} --> {avg_val_loss:.2f}).  Saving model ...")
        # 베스트 모델 저장
        torch.save(model.state_dict(), 'model_checkpoint.pt')
        # 최소 검증 손실 업데이트
        min_val_loss = avg_val_loss

"""## 모델 저장과 로드"""

# Commented out IPython magic to ensure Python compatibility.
# %pwd

# Commented out IPython magic to ensure Python compatibility.
# %ls -al

# 베스트 모델 로드
model.load_state_dict(torch.load("model_checkpoint.pt"))

"""# 테스트 데이터에 대한 평가"""

avg_val_loss, eval_metrics = evaluate(model, test_dataloader, device)
print("  Test Loss: {0:.2f}".format(avg_val_loss))
print("  Accuracy: {0:.2f}".format(eval_metrics['accuracy']))
print("  F1 Macro: {0:.2f}".format(eval_metrics['f1_macro']))
print("  F1 Micro: {0:.2f}".format(eval_metrics['f1_micro']))
print("  F1 Weighted: {0:.2f}".format(eval_metrics['f1_weighted']))

"""# 예측"""

from transformers import pipeline

pipe = pipeline("text-classification", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512,
                return_all_scores=True, function_to_apply='sigmoid')

result = pipe('틀니들은 왜 그렇게 민폐를 끼치냐? 특히 나이 먹은 남자들이 심하다')
print(result)

label_dict = {'LABEL_0' : '출신차별', 'LABEL_1' : '외모차별', 'LABEL_2' : '정치성향차별', \
              'LABEL_3': '혐오욕설', 'LABEL_4': '연령차별', 'LABEL_5': '성차별', 'LABEL_6' : '인종차별', \
              'LABEL_7': '종교차별', 'LABEL_8': '해당사항없음'}

def prediction(text):
  result = pipe(text)
  return [label_dict[res['label']] for res in result[0] if res['score'] > 0.5]

prediction('틀니들은 왜 그렇게 민폐를 끼치냐? 특히 나이 먹은 남자들이 심하다')