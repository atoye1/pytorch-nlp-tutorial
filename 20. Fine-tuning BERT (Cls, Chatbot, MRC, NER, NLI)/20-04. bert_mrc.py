# -*- coding: utf-8 -*-
"""20-04. bert_mrc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h611xngQZu-veYYsu1C7Nbbu6ox-B_Dm
"""

pip install transformers

!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -O KorQuAD_v1.0_train.json
!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -O KorQuAD_v1.0_dev.json

import os
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path
from transformers import BertTokenizerFast

def read_squad(path):
    path = Path(path)
    with open(path, 'rb') as f:
        squad_dict = json.load(f)

    contexts = []
    questions = []
    answers = []
    for group in squad_dict['data']:
        for passage in group['paragraphs']:
            context = passage['context']
            for qa in passage['qas']:
                question = qa['question']
                for answer in qa['answers']:
                    contexts.append(context)
                    questions.append(question)
                    answers.append(answer)

    return contexts, questions, answers

train_contexts, train_questions, train_answers = read_squad('KorQuAD_v1.0_train.json')
val_contexts, val_questions, val_answers = read_squad('KorQuAD_v1.0_dev.json')

print('훈련 데이터의 본문 개수 :', len(train_contexts))
print('훈련 데이터의 질문 개수 :', len(train_questions))
print('훈련 데이터의 답변 개수 :', len(train_answers))
print('테스트 데이터의 본문 개수 :', len(val_contexts))
print('테스트 데이터의 질문 개수 :', len(val_questions))
print('테스트 데이터의 답변 개수 :', len(val_answers))

print('첫번째 샘플의 본문')
print('-----------------')
print(train_contexts[0])

print('첫번째 샘플의 질문')
print('-----------------')
print(train_questions[0])

print('첫번째 샘플의 답변')
print('-----------------')
print(train_answers[0])

def add_end_idx(answers, contexts):
    for answer, context in zip(answers, contexts):
        # 뒤에 공백이 있다면 제거. ex)'1990년대 ' -> '1990년대'
        answer['text'] = answer['text'].rstrip()

        # 시작 인덱스에 정답의 길이를 더하여 종료 인덱스 계산
        gold_text = answer['text']
        start_idx = answer['answer_start']
        end_idx = start_idx + len(gold_text)

        assert context[start_idx:end_idx] == gold_text, "end_index 계산에 에러가 있습니다."
        answer['answer_end'] = end_idx

add_end_idx(train_answers, train_contexts)
add_end_idx(val_answers, val_contexts)

print('첫번째 샘플의 답변')
print('-----------------')
print(train_answers[0])

train_contexts[0][54]

train_contexts[0][55]

train_contexts[0][56]

train_contexts[0][54:57]

tokenizer = BertTokenizerFast.from_pretrained('klue/bert-base')

train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True, max_length=256)
val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True, max_length=256)

train_encodings[0]

print(train_encodings[0].ids)

print(train_encodings[0].tokens)

print(train_encodings[0].type_ids)

print(train_encodings[0].attention_mask)

def add_token_positions(encodings, answers):
    start_positions = []
    end_positions = []
    deleting_list = []

    for i in tqdm(range(len(answers))):
        # 토큰화 전 문자의 인덱스(start, end)로부터 토큰화한 후의 문자의 인덱스(start, end)를 찾아냄.
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))

        # 시작 인덱스가 비정상인 경우. 즉, 본문에 정답이 없는 경우
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length
            deleting_list.append(i)

        # 종료 인덱스가 비정상인 경우. 즉, 본문에 정답이 없는 경우
        if end_positions[-1] is None:
            end_positions[-1] = tokenizer.model_max_length
            if i not in deleting_list:
              deleting_list.append(i)

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})
    return deleting_list

deleting_list_for_train = add_token_positions(train_encodings, train_answers)
deleting_list_for_test = add_token_positions(val_encodings, val_answers)

print('삭제 예정인 훈련 샘플 :', deleting_list_for_train)
print('삭제 예정인 테스트 샘플 :', deleting_list_for_test)

print('761번 샘플의 질문 :',train_questions[761])
print('-'*50)
print('761번 샘플의 기존 원문 :',train_contexts[761])

print('761번 샘플의 기존 정답 :', train_answers[761])

print('761번 샘플 전처리 후 :', tokenizer.decode(train_encodings['input_ids'][761]))

def delete_samples(encodings, deleting_list):
  input_ids = np.delete(np.array(encodings['input_ids']), deleting_list, axis=0)
  attention_masks = np.delete(np.array(encodings['attention_mask']), deleting_list, axis=0)
  start_positions = np.delete(np.array(encodings['start_positions']), deleting_list, axis=0)
  end_positions = np.delete(np.array(encodings['end_positions']), deleting_list, axis=0)

  X_data = [input_ids, attention_masks]
  y_data = [start_positions, end_positions]

  return X_data, y_data

X_train, y_train = delete_samples(train_encodings, deleting_list_for_train)
X_test, y_test = delete_samples(val_encodings, deleting_list_for_test)

print('-------------삭제 전-------------')
print('훈련 데이터의 샘플의 개수 :', len(train_contexts))
print('테스트 데이터의 샘플의 개수 :', len(val_contexts))
print('-------------삭제 후-------------')
print('훈련 데이터의 샘플의 개수 :', len(X_train[0]))
print('테스트 데이터의 샘플의 개수 :', len(X_test[0]))

import torch
from torch import nn
from torch.optim import Adam
from torch.utils.data import TensorDataset, DataLoader
from transformers import BertModel, BertTokenizer

class BertForQuestionAnswering(nn.Module):
    # init에는 우리가 사용할 모델의 층들을 선언하는 부분.
    def __init__(self, model_name):
        super(BertForQuestionAnswering, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        # 사용할 출력층에서 사용할 뉴런은 2개. 각각 시작 인덱스 예측과 종료 인덱스 예측에 사용된다.
        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2)

    # forward 부분은 init에서 선언한 층들을 연결하는 부분.
    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        # BERT의 마지막 층의 모든 토큰들. outputs[0].shape == (배치 크기, 문장 길이, 768)
        # 예를 들어서 하나의 데이터가 512개의 단어로 구성되어져 있다면 (배치 크기, 512, 768)
        # 그런데 동시에 50개의 데이터를 처리한다면. 즉, 배치 크기가 50이라면 (50, 512, 768)
        sequence_output = outputs[0]

        # 사용할 출력층은 총 뉴런 2개. 각각 시작 인덱스 예측과 종료 인덱스 예측에 사용된다.
        logits = self.qa_outputs(sequence_output)

        # 뉴런 2개를 쪼갠다. 그래서 1개짜리가 2개.
        start_logits, end_logits = logits.split(1, dim=-1)

        start_logits = start_logits.squeeze(-1)
        end_logits = end_logits.squeeze(-1)

        # 시작 인덱스에 대한 다중 클래스 분류 문제.
        # 예를 들어 모델의 최대 길이가 512라면 512개 중 가장 확률값이 높은 1개를 찾는다.
        start_probs = torch.softmax(start_logits, dim=-1)

        # 종료 인덱스에 대한 다중 클래스 분류 문제
        # 예를 들어 모델의 최대 길이가 512라면 512개 중 가장 확률값이 높은 1개를 찾는다.
        end_probs = torch.softmax(end_logits, dim=-1)

        return start_probs, end_probs

model = BertForQuestionAnswering("klue/bert-base")
optimizer = Adam(model.parameters(), lr=5e-5)
loss = nn.CrossEntropyLoss()

def create_dataset(X_data, y_data):
    """
    입력 데이터(X_data)와 레이블(y_data)를 받아 파이토치의 TensorDataset으로 변환합니다.
    이 함수는 기계 독해 모델 학습을 위한 데이터셋 생성 과정에서 필요한 입력 데이터와 타겟 레이블을 텐서로 변환하고,
    이를 TensorDataset 객체로 묶어 반환합니다. 이렇게 생성된 데이터셋은 DataLoader를 통해 모델 학습에 사용될 수 있습니다.

    Parameters:
    X_data (tuple): 입력 데이터를 담고 있는 튜플. X_data는 (input_ids, attention_masks) 형태로,
                    각각 정수 인코딩된 텍스트와 어텐션 마스크 정보를 포함합니다.
    y_data (tuple): 타겟 레이블 데이터를 담고 있는 튜플. y_data는 (start_positions, end_positions) 형태로,
                    각각 정답이 시작하는 위치와 끝나는 위치의 인덱스 정보를 포함합니다.

    Returns:
    TensorDataset: 입력 데이터와 타겟 레이블을 포함하는 파이토치의 TensorDataset 객체.
    """
    # 입력 데이터(X_data)에서 정수 인코딩된 텍스트와 어텐션 마스크를 추출합니다.
    input_ids, attention_masks = X_data

    # 타겟 레이블(y_data)에서 정답의 시작 위치와 종료 위치를 추출합니다.
    start_positions, end_positions = y_data

    # 추출한 데이터들을 각각 파이토치 텐서로 변환합니다. 데이터 타입은 모두 torch.long으로 설정합니다.
    input_ids = torch.tensor(input_ids, dtype=torch.long)
    attention_masks = torch.tensor(attention_masks, dtype=torch.long)
    start_positions = torch.tensor(start_positions, dtype=torch.long)
    end_positions = torch.tensor(end_positions, dtype=torch.long)

    # 변환된 텐서들을 TensorDataset 객체로 묶어서 반환합니다. 이 객체는 DataLoader와 함께 사용될 수 있습니다.
    dataset = TensorDataset(input_ids, attention_masks, start_positions, end_positions)
    return dataset

batch_size = 50

train_dataset = create_dataset(X_train, y_train)
test_dataset = create_dataset(X_test, y_test)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

54565 / 50

len(train_dataloader)

def evaluate(model, dataloader, loss, device):
    """
    모델의 성능을 평가하고 데이터로더를 통해 손실 값을 계산합니다. 이 함수는 주어진 데이터로더에 대해 모델을 평가 모드로 설정하고,
    각 배치에 대한 손실을 계산하여 평균 손실을 반환합니다.

    Parameters:
    model (torch.nn.Module): 평가할 모델 객체.
    dataloader (torch.utils.data.DataLoader): 평가 데이터셋의 DataLoader.
    loss (torch.nn.modules.loss): 손실 함수.
    device (torch.device): 계산에 사용할 장치 (예: 'cuda' 또는 'cpu').

    Returns:
    float: 데이터로더에 있는 모든 데이터에 대한 평균 손실값.
    """
    model.eval() # 모델을 평가 모드로 설정
    total_loss = 0.0

    for input_ids, attention_mask, start_positions, end_positions in dataloader:
        # 데이터를 지정된 장치로 이동
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        start_positions = start_positions.to(device)
        end_positions = end_positions.to(device)

        with torch.no_grad(): # 그라디언트 계산을 비활성화
            # 모델의 예측 결과를 얻음
            start_probs, end_probs = model(input_ids, attention_mask=attention_mask)
            # 시작 위치와 종료 위치에 대한 손실 계산
            loss_start = loss(start_probs, start_positions)
            loss_end = loss(end_probs, end_positions)
            # 손실의 평균을 계산
            batch_loss = (loss_start + loss_end) / 2
            # 총 손실에 배치 손실 추가
            total_loss += batch_loss.item()

    # 평균 손실 계산
    avg_loss = total_loss / len(dataloader)
    return avg_loss

# BERT 기반의 기계 독해 모델 학습 과정을 실행하는 코드입니다.
for epoch in range(3):  # 총 3 에포크(epoch) 동안 학습을 수행합니다.
    model.train()  # 모델을 학습 모드로 설정합니다.
    total_loss = 0.0  # 에포크별 총 손실을 초기화합니다.

    # tqdm을 사용하여 학습 진행 상황을 시각화하며, train_dataloader의 각 배치(batch)를 순회합니다.
    for input_ids, attention_mask, start_positions, end_positions in tqdm(train_dataloader, total=len(train_dataloader)):
        # 배치 데이터를 현재 설정된 장치(device)로 이동합니다. GPU나 CPU 사용에 따라 달라집니다.
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        start_positions = start_positions.to(device)
        end_positions = end_positions.to(device)

        # 모델에 입력 데이터를 제공하고 시작 및 종료 위치에 대한 예측 확률을 계산합니다.
        start_probs, end_probs = model(input_ids, attention_mask=attention_mask)
        # 손실 함수를 사용하여 예측된 시작 및 종료 위치의 손실을 계산합니다.
        loss_start = loss(start_probs, start_positions)
        loss_end = loss(end_probs, end_positions)

        # 시작 위치와 종료 위치에 대한 손실의 평균을 이 배치의 최종 손실로 계산합니다.
        batch_loss = (loss_start + loss_end) / 2

        optimizer.zero_grad()  # 옵티마이저의 그라디언트를 초기화합니다.
        batch_loss.backward()  # 손실에 대해 역전파를 수행하여 그라디언트를 계산합니다.
        optimizer.step()  # 계산된 그라디언트를 사용하여 모델의 매개변수를 업데이트합니다.

        # 에포크별 총 손실에 이 배치의 손실을 누적합니다.
        total_loss += batch_loss.item()

    # 에포크가 끝날 때마다, 평균 손실을 계산하고 출력합니다.
    avg_loss = total_loss / len(train_dataloader)  # 총 손실을 배치의 수로 나누어 평균 손실을 계산합니다.
    print(f"Epoch {epoch + 1}, Loss: {avg_loss}")  # 현재 에포크의 평균 손실을 출력합니다.

    # 학습된 모델을 평가 모드로 설정하고, 검증 데이터셋에 대한 손실을 계산합니다.
    val_loss = evaluate(model, test_dataloader, loss, device)  # 검증 데이터셋에 대한 평균 손실을 계산하는 함수를 호출합니다.
    print(f"Epoch {epoch + 1}, Val Loss: {val_loss}")  # 검증 데이터셋에 대한 평균 손실을 출력합니다.

def predict(model, input_ids, attention_mask):
    """
    주어진 입력에 대해 모델이 예측한 시작 인덱스와 종료 인덱스를 반환합니다. 이 함수는 모델을 평가 모드로 설정하고,
    입력 텐서에 대한 예측을 수행한 후, 시작 위치와 종료 위치의 확률 분포에서 가장 높은 확률을 가진 인덱스를 각각 찾아 반환합니다.

    Parameters:
    model (torch.nn.Module): 예측을 수행할 모델 객체.
    input_ids (torch.Tensor): 모델에 입력될 토큰화된 텍스트의 정수 인코딩을 포함하는 텐서.
    attention_mask (torch.Tensor): 입력 텐서에서 실제 텍스트와 패딩을 구분하기 위한 마스크 텐서.

    Returns:
    tuple: (start_index, end_index) 형태로, 모델이 예측한 시작 인덱스와 종료 인덱스를 반환합니다.
    """
    model.eval()  # 모델을 평가 모드로 설정합니다.
    with torch.no_grad():  # 그라디언트 계산을 비활성화합니다.
        # 모델에 입력을 제공하기 전에 입력의 차원을 증가시키고, 계산에 사용할 장치로 이동합니다.
        start_probs, end_probs = model(input_ids.unsqueeze(0).to(device),
                                       attention_mask=attention_mask.unsqueeze(0).to(device))

    # 시작 위치와 종료 위치에 대한 확률 분포에서 가장 높은 확률을 가진 인덱스를 찾습니다.
    start_index = torch.argmax(start_probs).item()  # 시작 인덱스 추출
    end_index = torch.argmax(end_probs).item()  # 종료 인덱스 추출

    return start_index, end_index  # 추출된 시작 및 종료 인덱스를 반환합니다.

# 테스트 데이터, 출력할 테스트 데이터의 인덱스, 토크나이저를 받아서 해당 인덱스 샘플의 결과를 출력
def display_output(test_data, index, tokenizer):


    # index 번호의 테스트 데이터 샘플을 얻는다.
    input_ids, attention_mask, start_position, end_position = test_data[index]

    #------- 임의의 index로부터 테스트 데이터의 질문(question), 본문(context), 정답(true_answer)을 추출하는 과정 -------#
    # decoded_text는 [CLS] 본문 [SEP] 질문 [SEP]의 형태로 구성된 텍스트.
    decoded_text = tokenizer.decode(input_ids)

    # decoded_text로부터 정답 문자열 추출.
    true_answer = tokenizer.decode(input_ids[start_position:end_position + 1])

    # 본문과 질문 추출하여 각각 context, question에 저장.
    context = decoded_text.split('[SEP]')[0].replace('[CLS]', '').strip()
    question = decoded_text.split('[SEP]')[1].strip()
    #-------------------------------------------------------------------------------------------------------------------#

    # 정수 인코딩 입력과 어텐션 마스크로부터 모델의 예측을 얻는다.
    start_index, end_index = predict(model, input_ids=input_ids, attention_mask=attention_mask)
    predicted_answer = tokenizer.decode(input_ids[start_index:end_index + 1])

    # 본문, 질문, 정답 그리고 모델의 예측을 출력하여 정답과 예측값을 비교한다.
    print(f"본문: {context}")
    print(f"질문: {question}")
    print(f"정답: {true_answer}")
    print(f"예측: {predicted_answer}")

display_output(test_dataset, 15, tokenizer)

index = 5

display_output(test_dataset, index, tokenizer)

for i in range(0, 100):
  display_output(test_dataset, i, tokenizer)
  print('-' * 100)