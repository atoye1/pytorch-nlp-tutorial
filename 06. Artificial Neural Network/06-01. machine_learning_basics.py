# -*- coding: utf-8 -*-
"""06-01. machine_learning_basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wp9cFKGS54N4uGuyCY_TNGB8FopDhGO3

## 7. 비선형 활성화 함수
"""

import numpy as np # 넘파이 사용
import matplotlib.pyplot as plt # 맷플롯립 사용

"""### 2) 시그모이드 함수(Sigmoid function)와 기울기 소실"""

# 시그모이드 함수 그래프를 그리는 코드
def sigmoid(x):
    return 1/(1+np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()

"""### 3) 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)"""

x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.tanh(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,-1.0], ':')
plt.axhline(y=0, color='orange', linestyle='--')
plt.title('Tanh Function')
plt.show()

"""### 4) 렐루 함수(ReLU)"""

def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Relu Function')
plt.show()

"""### 5) 리키 렐루(Leaky ReLU)"""

a = 0.1

def leaky_relu(x):
    return np.maximum(a*x, x)

x = np.arange(-5.0, 5.0, 0.1)
y = leaky_relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Leaky ReLU Function')
plt.show()

"""### 6) 소프트맥스 함수(Softamx function)"""

x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.exp(x) / np.sum(np.exp(x))

plt.plot(x, y)
plt.title('Softmax Function')
plt.show()