{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "xpasq5ERm69W"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()"
      ],
      "metadata": {
        "id": "MRwazvDhm7GS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(sentence))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBxosNV8m8HZ",
        "outputId": "ab35eb90-e754-4135-9ce2-fe81f0edacb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['memory', 'is', 'the', 'for', 'Repeat', 'best', 'medicine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
        "word2index['<unk>']=0"
      ],
      "metadata": {
        "id": "GHeWAAU6m88h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2index['memory'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz9SO6kbm90o",
        "outputId": "1ce0dcc4-e2cd-4072-b8f4-192bff84bd85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJBtDqSim-t4",
        "outputId": "296cd658-0fd7-47a6-b92b-1516900e4fe2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'memory', 2: 'is', 3: 'the', 4: 'for', 5: 'Repeat', 6: 'best', 7: 'medicine', 0: '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(index2word[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQn7pr1ym_iI",
        "outputId": "46ed8f99-44cd-43a3-ee5d-2e8d73bfb4b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환.\n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    return input_seq, label_seq"
      ],
      "metadata": {
        "id": "6y80fQDRnAWw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = build_data(sentence, word2index)"
      ],
      "metadata": {
        "id": "qrNb-nBlnBcY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "metadata": {
        "id": "KxbsCgBMnCdA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기"
      ],
      "metadata": {
        "id": "Yhd1dZxLnDrY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "# 손실함수 정의\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
        "# 옵티마이저 정의\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "metadata": {
        "id": "SZv75fnGnEmg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n",
        "output = model(X)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwDr4L6OnFcQ",
        "outputId": "4dbbcb74-8c51-420c-e3f3-ed9ce3b7cf15"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3396, -0.0498,  0.5445,  0.0155,  0.2364,  0.4760, -0.2998, -0.0805],\n",
            "        [-0.0769, -0.1373,  0.5565,  0.0573,  0.1828,  0.5001, -0.1645,  0.0317],\n",
            "        [-0.2831,  0.1319,  0.1576, -0.0971, -0.0756,  0.4306,  0.0078, -0.1124],\n",
            "        [-0.3625,  0.2251,  0.3956, -0.0877, -0.1193,  0.3274, -0.1035, -0.1773],\n",
            "        [-0.4791, -0.0161,  0.4765, -0.1165, -0.0812,  0.6174, -0.3888, -0.2633],\n",
            "        [-0.2883,  0.1755,  0.4422, -0.0913, -0.1342,  0.4562, -0.4074, -0.3320]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbQlEMtQnGcA",
        "outputId": "a80e9089-d125-499a-e6fe-00c389f10436"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치화된 데이터를 단어로 전환하는 함수\n",
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "metadata": {
        "id": "mavmk05VnHSo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 시작\n",
        "for step in range(201):\n",
        "    # 경사 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 순방향 전파\n",
        "    output = model(X)\n",
        "    # 손실값 계산\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    # 역방향 전파\n",
        "    loss.backward()\n",
        "    # 매개변수 업데이트\n",
        "    optimizer.step()\n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIl5uyXnIP5",
        "outputId": "7bb7e311-18f8-4802-833b-a8f6bec9ae04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/201] 2.0619 \n",
            "Repeat is is Repeat is Repeat Repeat\n",
            "\n",
            "[41/201] 1.4778 \n",
            "Repeat is the best is for memory\n",
            "\n",
            "[81/201] 0.7828 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.3735 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.1978 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1201 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ]
    }
  ]
}