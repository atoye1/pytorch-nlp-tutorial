{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'great', 'work'], ['supreme', 'nice', 'quality'], ['bad'], ['nice', 'highly', 'respectable']]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent great work', 'supreme nice quality', 'bad', 'nice highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]\n",
    "\n",
    "tokenized_sentences = [sent.split() for sent in sentences]\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'nice': 3,\n",
       "         'great': 2,\n",
       "         'best': 1,\n",
       "         'amazing': 1,\n",
       "         'stop': 1,\n",
       "         'lies': 1,\n",
       "         'pitiful': 1,\n",
       "         'nerd': 1,\n",
       "         'excellent': 1,\n",
       "         'work': 1,\n",
       "         'supreme': 1,\n",
       "         'quality': 1,\n",
       "         'bad': 1,\n",
       "         'highly': 1,\n",
       "         'respectable': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = [w for sent in tokenized_sentences for w in sent]\n",
    "word_list\n",
    "word_counts = Counter(word_list)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'nice': 2,\n",
       " 'great': 3,\n",
       " 'best': 4,\n",
       " 'amazing': 5,\n",
       " 'stop': 6,\n",
       " 'lies': 7,\n",
       " 'pitiful': 8,\n",
       " 'nerd': 9,\n",
       " 'excellent': 10,\n",
       " 'work': 11,\n",
       " 'supreme': 12,\n",
       " 'quality': 13,\n",
       " 'bad': 14,\n",
       " 'highly': 15,\n",
       " 'respectable': 16}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "for word in word_counts:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 3, 11], [12, 2, 13], [14], [2, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "    seq = []\n",
    "    for sent in tokenized_sentences:\n",
    "        seq.append([word_to_index.get(w, 1) for w in sent])\n",
    "    return seq\n",
    "X_encoded = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 0], [6, 7, 0, 0, 0], [8, 9, 0, 0, 0], [10, 3, 11, 0, 0], [12, 2, 13, 0, 0], [14, 0, 0, 0, 0], [2, 15, 16, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(X_encoded, maxlen):\n",
    "    padded_X = []\n",
    "    for seq in X_encoded:\n",
    "        if len(seq) < maxlen:\n",
    "            seq = seq + [0] * (maxlen - len(seq))\n",
    "        else:\n",
    "            seq = seq[:maxlen]\n",
    "        padded_X.append(seq)\n",
    "    return padded_X\n",
    "X_train = pad_sequences(X_encoded, 5)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * 5, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        flattend = self.flatten(embedded)\n",
    "        output = self.fc(flattend)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_dim = 100\n",
    "simple_model = SimpleModel(len(word_to_index), embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(simple_model.parameters())\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.624923586845398\n",
      "Epoch 2, Loss: 0.4315648078918457\n",
      "Epoch 3, Loss: 0.28908026218414307\n",
      "Epoch 4, Loss: 0.20004642009735107\n",
      "Epoch 5, Loss: 0.14668801426887512\n",
      "Epoch 6, Loss: 0.11396041512489319\n",
      "Epoch 7, Loss: 0.09262114018201828\n",
      "Epoch 8, Loss: 0.07758186757564545\n",
      "Epoch 9, Loss: 0.06618209928274155\n",
      "Epoch 10, Loss: 0.05708176642656326\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = simple_model(inputs).view(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5895,  0.1298, -0.6350,  ..., -2.0524,  2.4095, -0.1532],\n",
       "        [-1.0700, -1.1878, -1.0865,  ..., -1.5663, -0.8975, -0.1144],\n",
       "        [-0.3585, -0.4442,  0.1917,  ..., -0.4807,  0.2418, -0.4552],\n",
       "        ...,\n",
       "        [-0.7217,  0.9467,  2.2711,  ..., -1.0715,  0.9850, -0.3917],\n",
       "        [ 0.7121, -0.7582,  1.1838,  ..., -0.6445, -1.8391,  0.0964],\n",
       "        [ 1.0371,  1.1644,  0.3995,  ..., -1.1793, -0.7625,  0.9683]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류를 위한 샘플 데이터셋 (50개 문장: 25개 긍정, 25개 부정)\n",
    "sample_texts = [\n",
    "    # 긍정적인 문장 (25개)\n",
    "    \"This movie was absolutely amazing, I loved every second of it!\",\n",
    "    \"The restaurant's food was delicious and the service was outstanding.\",\n",
    "    \"I had the best experience with this product, highly recommend it to everyone.\",\n",
    "    \"The concert was incredible, the band performed beyond my expectations.\",\n",
    "    \"This book changed my perspective on life, truly inspiring.\",\n",
    "    \"The customer service team was very helpful and resolved my issue quickly.\",\n",
    "    \"The new software update has significantly improved performance.\",\n",
    "    \"I'm extremely satisfied with my purchase, worth every penny.\",\n",
    "    \"The hotel room was clean, comfortable, and had a beautiful view.\",\n",
    "    \"My vacation was perfect from start to finish, couldn't have asked for more.\",\n",
    "    \"The team's collaboration made the project a huge success.\",\n",
    "    \"This phone exceeds all my expectations, best device I've ever owned.\",\n",
    "    \"The online course was well-structured and provided valuable knowledge.\",\n",
    "    \"The scenery at the national park was breathtaking and unforgettable.\",\n",
    "    \"This coffee shop serves the most delicious pastries in town.\",\n",
    "    \"The instructor was knowledgeable and made learning enjoyable.\",\n",
    "    \"I received my package earlier than expected, fantastic service!\",\n",
    "    \"The mobile app is user-friendly and makes tasks much easier.\",\n",
    "    \"The theater performance moved me to tears, absolutely brilliant.\",\n",
    "    \"My new car drives smoothly and has amazing fuel efficiency.\",\n",
    "    \"The gym facilities are modern and the trainers are professional.\",\n",
    "    \"This streaming service offers an incredible selection of movies.\",\n",
    "    \"The wedding venue was perfect and created wonderful memories.\",\n",
    "    \"The tech support solved my complex problem in minutes.\",\n",
    "    \"The conference was informative and provided great networking opportunities.\",\n",
    "    \n",
    "    # 부정적인 문장 (25개)\n",
    "    \"This movie was terrible, I wasted two hours of my life.\",\n",
    "    \"The food was cold and the waiter was extremely rude.\",\n",
    "    \"The product broke after just two days, terrible quality.\",\n",
    "    \"The concert was disappointing, the sound quality was awful.\",\n",
    "    \"This book was boring and predictable, wouldn't recommend it.\",\n",
    "    \"Customer service was unhelpful and ignored my complaints.\",\n",
    "    \"The software update crashed my computer and lost my files.\",\n",
    "    \"I regret buying this overpriced and underperforming product.\",\n",
    "    \"The hotel room was dirty, noisy, and nothing like the pictures.\",\n",
    "    \"My vacation was ruined by bad weather and poor planning.\",\n",
    "    \"The team failed to meet deadlines and the project was a disaster.\",\n",
    "    \"This phone constantly freezes and the battery life is terrible.\",\n",
    "    \"The online course was disorganized and contained outdated information.\",\n",
    "    \"The park was overcrowded and filled with litter everywhere.\",\n",
    "    \"This coffee shop serves the worst coffee I've ever tasted.\",\n",
    "    \"The instructor was unprepared and couldn't answer basic questions.\",\n",
    "    \"My package arrived damaged and items were missing.\",\n",
    "    \"The app is frustrating to use and crashes constantly.\",\n",
    "    \"The performance was amateur and not worth the ticket price.\",\n",
    "    \"My new car has been in the repair shop more than on the road.\",\n",
    "    \"The gym equipment is outdated and often out of order.\",\n",
    "    \"This streaming service buffers constantly despite my fast internet.\",\n",
    "    \"The venue was too small and uncomfortable for the event.\",\n",
    "    \"Tech support kept me on hold for hours without resolving my issue.\",\n",
    "    \"The conference was a waste of time and money, learned nothing new.\"\n",
    "]\n",
    "\n",
    "# 레이블 (1: 긍정, 0: 부정)\n",
    "sample_labels = [1] * 25 + [0] * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a6495567ad0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^%\\w\\s]', '', text)\n",
    "    return text\n",
    "processed_texts = [preprocess_text(text) for text in sample_texts]\n",
    "tokenized_texts = [text.split() for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 고유 단어 수: 261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'the': 46,\n",
       "         'and': 29,\n",
       "         'was': 28,\n",
       "         'my': 18,\n",
       "         'this': 12,\n",
       "         'service': 6,\n",
       "         'i': 5,\n",
       "         'of': 5,\n",
       "         'to': 5,\n",
       "         'new': 4,\n",
       "         'a': 4,\n",
       "         'is': 4,\n",
       "         'it': 3,\n",
       "         'with': 3,\n",
       "         'product': 3,\n",
       "         'on': 3,\n",
       "         'life': 3,\n",
       "         'has': 3,\n",
       "         'performance': 3,\n",
       "         'for': 3,\n",
       "         'coffee': 3,\n",
       "         'shop': 3,\n",
       "         'in': 3,\n",
       "         'terrible': 3,\n",
       "         'constantly': 3,\n",
       "         'movie': 2,\n",
       "         'absolutely': 2,\n",
       "         'amazing': 2,\n",
       "         'every': 2,\n",
       "         'food': 2,\n",
       "         'delicious': 2,\n",
       "         'had': 2,\n",
       "         'best': 2,\n",
       "         'recommend': 2,\n",
       "         'concert': 2,\n",
       "         'incredible': 2,\n",
       "         'expectations': 2,\n",
       "         'book': 2,\n",
       "         'customer': 2,\n",
       "         'team': 2,\n",
       "         'issue': 2,\n",
       "         'software': 2,\n",
       "         'update': 2,\n",
       "         'extremely': 2,\n",
       "         'worth': 2,\n",
       "         'hotel': 2,\n",
       "         'room': 2,\n",
       "         'vacation': 2,\n",
       "         'perfect': 2,\n",
       "         'couldnt': 2,\n",
       "         'more': 2,\n",
       "         'made': 2,\n",
       "         'project': 2,\n",
       "         'phone': 2,\n",
       "         'ive': 2,\n",
       "         'ever': 2,\n",
       "         'online': 2,\n",
       "         'course': 2,\n",
       "         'provided': 2,\n",
       "         'park': 2,\n",
       "         'serves': 2,\n",
       "         'instructor': 2,\n",
       "         'package': 2,\n",
       "         'than': 2,\n",
       "         'app': 2,\n",
       "         'me': 2,\n",
       "         'car': 2,\n",
       "         'gym': 2,\n",
       "         'are': 2,\n",
       "         'streaming': 2,\n",
       "         'venue': 2,\n",
       "         'tech': 2,\n",
       "         'support': 2,\n",
       "         'conference': 2,\n",
       "         'two': 2,\n",
       "         'hours': 2,\n",
       "         'quality': 2,\n",
       "         'nothing': 2,\n",
       "         'outdated': 2,\n",
       "         'loved': 1,\n",
       "         'second': 1,\n",
       "         'restaurants': 1,\n",
       "         'outstanding': 1,\n",
       "         'experience': 1,\n",
       "         'highly': 1,\n",
       "         'everyone': 1,\n",
       "         'band': 1,\n",
       "         'performed': 1,\n",
       "         'beyond': 1,\n",
       "         'changed': 1,\n",
       "         'perspective': 1,\n",
       "         'truly': 1,\n",
       "         'inspiring': 1,\n",
       "         'very': 1,\n",
       "         'helpful': 1,\n",
       "         'resolved': 1,\n",
       "         'quickly': 1,\n",
       "         'significantly': 1,\n",
       "         'improved': 1,\n",
       "         'im': 1,\n",
       "         'satisfied': 1,\n",
       "         'purchase': 1,\n",
       "         'penny': 1,\n",
       "         'clean': 1,\n",
       "         'comfortable': 1,\n",
       "         'beautiful': 1,\n",
       "         'view': 1,\n",
       "         'from': 1,\n",
       "         'start': 1,\n",
       "         'finish': 1,\n",
       "         'have': 1,\n",
       "         'asked': 1,\n",
       "         'teams': 1,\n",
       "         'collaboration': 1,\n",
       "         'huge': 1,\n",
       "         'success': 1,\n",
       "         'exceeds': 1,\n",
       "         'all': 1,\n",
       "         'device': 1,\n",
       "         'owned': 1,\n",
       "         'wellstructured': 1,\n",
       "         'valuable': 1,\n",
       "         'knowledge': 1,\n",
       "         'scenery': 1,\n",
       "         'at': 1,\n",
       "         'national': 1,\n",
       "         'breathtaking': 1,\n",
       "         'unforgettable': 1,\n",
       "         'most': 1,\n",
       "         'pastries': 1,\n",
       "         'town': 1,\n",
       "         'knowledgeable': 1,\n",
       "         'learning': 1,\n",
       "         'enjoyable': 1,\n",
       "         'received': 1,\n",
       "         'earlier': 1,\n",
       "         'expected': 1,\n",
       "         'fantastic': 1,\n",
       "         'mobile': 1,\n",
       "         'userfriendly': 1,\n",
       "         'makes': 1,\n",
       "         'tasks': 1,\n",
       "         'much': 1,\n",
       "         'easier': 1,\n",
       "         'theater': 1,\n",
       "         'moved': 1,\n",
       "         'tears': 1,\n",
       "         'brilliant': 1,\n",
       "         'drives': 1,\n",
       "         'smoothly': 1,\n",
       "         'fuel': 1,\n",
       "         'efficiency': 1,\n",
       "         'facilities': 1,\n",
       "         'modern': 1,\n",
       "         'trainers': 1,\n",
       "         'professional': 1,\n",
       "         'offers': 1,\n",
       "         'an': 1,\n",
       "         'selection': 1,\n",
       "         'movies': 1,\n",
       "         'wedding': 1,\n",
       "         'created': 1,\n",
       "         'wonderful': 1,\n",
       "         'memories': 1,\n",
       "         'solved': 1,\n",
       "         'complex': 1,\n",
       "         'problem': 1,\n",
       "         'minutes': 1,\n",
       "         'informative': 1,\n",
       "         'great': 1,\n",
       "         'networking': 1,\n",
       "         'opportunities': 1,\n",
       "         'wasted': 1,\n",
       "         'cold': 1,\n",
       "         'waiter': 1,\n",
       "         'rude': 1,\n",
       "         'broke': 1,\n",
       "         'after': 1,\n",
       "         'just': 1,\n",
       "         'days': 1,\n",
       "         'disappointing': 1,\n",
       "         'sound': 1,\n",
       "         'awful': 1,\n",
       "         'boring': 1,\n",
       "         'predictable': 1,\n",
       "         'wouldnt': 1,\n",
       "         'unhelpful': 1,\n",
       "         'ignored': 1,\n",
       "         'complaints': 1,\n",
       "         'crashed': 1,\n",
       "         'computer': 1,\n",
       "         'lost': 1,\n",
       "         'files': 1,\n",
       "         'regret': 1,\n",
       "         'buying': 1,\n",
       "         'overpriced': 1,\n",
       "         'underperforming': 1,\n",
       "         'dirty': 1,\n",
       "         'noisy': 1,\n",
       "         'like': 1,\n",
       "         'pictures': 1,\n",
       "         'ruined': 1,\n",
       "         'by': 1,\n",
       "         'bad': 1,\n",
       "         'weather': 1,\n",
       "         'poor': 1,\n",
       "         'planning': 1,\n",
       "         'failed': 1,\n",
       "         'meet': 1,\n",
       "         'deadlines': 1,\n",
       "         'disaster': 1,\n",
       "         'freezes': 1,\n",
       "         'battery': 1,\n",
       "         'disorganized': 1,\n",
       "         'contained': 1,\n",
       "         'information': 1,\n",
       "         'overcrowded': 1,\n",
       "         'filled': 1,\n",
       "         'litter': 1,\n",
       "         'everywhere': 1,\n",
       "         'worst': 1,\n",
       "         'tasted': 1,\n",
       "         'unprepared': 1,\n",
       "         'answer': 1,\n",
       "         'basic': 1,\n",
       "         'questions': 1,\n",
       "         'arrived': 1,\n",
       "         'damaged': 1,\n",
       "         'items': 1,\n",
       "         'were': 1,\n",
       "         'missing': 1,\n",
       "         'frustrating': 1,\n",
       "         'use': 1,\n",
       "         'crashes': 1,\n",
       "         'amateur': 1,\n",
       "         'not': 1,\n",
       "         'ticket': 1,\n",
       "         'price': 1,\n",
       "         'been': 1,\n",
       "         'repair': 1,\n",
       "         'road': 1,\n",
       "         'equipment': 1,\n",
       "         'often': 1,\n",
       "         'out': 1,\n",
       "         'order': 1,\n",
       "         'buffers': 1,\n",
       "         'despite': 1,\n",
       "         'fast': 1,\n",
       "         'internet': 1,\n",
       "         'too': 1,\n",
       "         'small': 1,\n",
       "         'uncomfortable': 1,\n",
       "         'event': 1,\n",
       "         'kept': 1,\n",
       "         'hold': 1,\n",
       "         'without': 1,\n",
       "         'resolving': 1,\n",
       "         'waste': 1,\n",
       "         'time': 1,\n",
       "         'money': 1,\n",
       "         'learned': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [word for text in tokenized_texts for word in text]\n",
    "word_counts = Counter(all_words)\n",
    "print(f\"총 고유 단어 수: {len(word_counts)}\")\n",
    "word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list(word_counts)\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in word_list:\n",
    "    if word not in word_to_idx:\n",
    "        word_to_idx[word] = len(word_to_index)\n",
    "word_to_idx['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 인덱스 시퀀스로 변환\n",
    "def text_to_sequence(text, word_to_idx, max_len=30):\n",
    "    words = preprocess_text(text).split()\n",
    "    sequence = [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in words]\n",
    "    \n",
    "    # 시퀀스 길이 조정 (패딩 또는 자르기)\n",
    "    if len(sequence) < max_len:\n",
    "        sequence = sequence + [word_to_idx[\"<PAD>\"]] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# 모든 샘플을 인덱스 시퀀스로 변환\n",
    "max_seq_length = 30  # 최대 시퀀스 길이 설정\n",
    "X = np.array([text_to_sequence(text, word_to_idx, max_seq_length) for text in sample_texts])\n",
    "y = np.array(sample_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: (40, 30), 레이블: (40,)\n",
      "검증 데이터 크기: (10, 30), 레이블: (10,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할: 학습 80%, 검증 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"학습 데이터 크기: {X_train.shape}, 레이블: {y_train.shape}\")\n",
    "print(f\"검증 데이터 크기: {X_val.shape}, 레이블: {y_val.shape}\")\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.LongTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "batch_size = 8  # 작은 데이터셋이므로 작은 배치 크기 사용\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.LongTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "batch_size =8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(word_to_idx)\n",
    "embedding_dim = 50  # 작은 데이터셋에 적합한 작은 차원\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # 이진 분류\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
