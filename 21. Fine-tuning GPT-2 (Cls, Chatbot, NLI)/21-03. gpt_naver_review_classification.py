# -*- coding: utf-8 -*-
"""21-03. gpt_naver_review_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uIBk-TAJ_TNK5tTMVPy0f3jWlYi6zgLW
"""

pip install transformers

import transformers
transformers.__version__

import pandas as pd
import numpy as np
import urllib.request
import os
from tqdm import tqdm
from transformers import AutoTokenizer, GPT2Model

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")

train_data = pd.read_table('ratings_train.txt')
test_data = pd.read_table('ratings_test.txt')

print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력

print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력

train_data[:5] # 상위 5개 출력

test_data[:5] # 상위 5개 출력

train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거
train_data = train_data.reset_index(drop=True)
print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인

test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거
test_data = test_data.reset_index(drop=True)
print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인

print(len(train_data))

print(len(test_data))

tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')

print(tokenizer.encode("보는내내 그대로 들어맞는 예측 카리스마 없는 악역"))

print(tokenizer.tokenize("보는내내 그대로 들어맞는 예측 카리스마 없는 악역"))

tokenizer.decode(tokenizer.encode("보는내내 그대로 들어맞는 예측 카리스마 없는 악역"))

for elem in tokenizer.encode("보는내내 그대로 들어맞는 예측 카리스마 없는 악역"):
  print(tokenizer.decode(elem))

print(tokenizer.tokenize("전율을 일으키는 영화. 다시 보고싶은 영화"))

print(tokenizer.encode("전율을 일으키는 영화. 다시 보고싶은 영화"))

for elem in tokenizer.encode("전율을 일으키는 영화. 다시 보고싶은 영화"):
  print(tokenizer.decode(elem))

for elem in tokenizer.encode("happy birthday~!"):
  print(tokenizer.decode(elem))

print(tokenizer.decode(3))

max_seq_len = 128

encoded_result = tokenizer.encode("전율을 일으키는 영화. 다시 보고싶은 영화", max_length=max_seq_len, pad_to_max_length=True)
print(encoded_result)
print('길이 :', len(encoded_result))

def pad_sequences(sentences, max_len):
  features = np.zeros((len(sentences), max_len), dtype=int)
  for index, sentence in enumerate(sentences):
    if len(sentence) != 0:
      features[index, :len(sentence)] = np.array(sentence)[:max_len]
  return features

def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):

    input_ids, data_labels = [], []

    for example, label in tqdm(zip(examples, labels), total=len(examples)):

        bos_token = [tokenizer.bos_token]
        eos_token = [tokenizer.eos_token]
        tokens = bos_token + tokenizer.tokenize(example) + eos_token
        input_id = tokenizer.convert_tokens_to_ids(tokens)
        input_id = pad_sequences([input_id], max_len=max_seq_len)[0]

        assert len(input_id) == max_seq_len, "Error with input length {} vs {}".format(len(input_id), max_seq_len)
        input_ids.append(input_id)
        data_labels.append(label)

    input_ids = torch.tensor(input_ids)
    data_labels = torch.tensor(data_labels)

    return input_ids, data_labels

train_X, train_y = convert_examples_to_features(train_data['document'], train_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)

test_X, test_y = convert_examples_to_features(test_data['document'], test_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)

# 최대 길이: 128
input_id = train_X[0]
label = train_y[0]

print('단어에 대한 정수 인코딩 :',input_id)
print('각 인코딩의 길이 :', len(input_id))
print('정수 인코딩 복원 :',tokenizer.decode(input_id))
print('레이블 :',label)

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from transformers import GPT2Model

class GPT2ForSequenceClassification(nn.Module):
    def __init__(self, model_name):
        super(GPT2ForSequenceClassification, self).__init__()
        self.gpt = GPT2Model.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.2)
        self.classifier = nn.Linear(self.gpt.config.hidden_size, 1)
        self.activation = nn.Sigmoid()

    def forward(self, inputs):
        outputs = self.gpt(inputs)[0]
        cls_token = outputs[:, -1, :]
        cls_token = self.dropout(cls_token)
        prediction = self.activation(self.classifier(cls_token))

        return prediction

class MyDataset(Dataset):
    def __init__(self, inputs, labels):
        self.inputs = inputs
        self.labels = labels

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input_tensor = torch.tensor(self.inputs[idx])
        label_tensor = torch.tensor(self.labels[idx])
        return input_tensor, label_tensor

# train_dataset = MyDataset(train_X, train_y)

train_dataset = TensorDataset(train_X, train_y)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

test_dataset = TensorDataset(test_X, test_y)
test_loader = DataLoader(test_dataset, batch_size=32)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GPT2ForSequenceClassification("skt/kogpt2-base-v2")
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)
loss = nn.BCELoss()

model.train()
for epoch in range(2):
    for inputs, labels in tqdm(train_loader, total=len(train_loader)):
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        batch_loss = loss(outputs.squeeze(), labels.float())
        batch_loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        total_loss = 0
        correct = 0
        total = 0

        for inputs, labels in tqdm(test_loader, total=len(test_loader)):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            batch_loss = loss(outputs.squeeze(), labels.float())
            total_loss += batch_loss.item()

            predicted = (outputs >= 0.5).long()
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        accuracy = correct / total
        average_loss = total_loss / len(test_loader)

    print(f"Epoch: {epoch+1}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}")

from transformers import pipeline

classification_pipeline = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    framework="pt"
)

input_text = "아 이 영화 개노잼이네 ㅋㅋ"

# Perform classification
result = classification_pipeline(input_text)

# Get the predicted label and score
predicted_label = result[0]['label']
score = result[0]['score']

# Print the result
print(f"Input Text: {input_text}")
print(f"Predicted Label: {predicted_label}")
print(f"Score: {score}")